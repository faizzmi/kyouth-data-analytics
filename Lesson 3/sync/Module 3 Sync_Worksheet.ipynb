{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f6a4c8",
   "metadata": {
    "id": "13f6a4c8"
   },
   "source": [
    "# Guided Practice Module 3: Data Preparation with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49599595",
   "metadata": {
    "id": "49599595"
   },
   "source": [
    "## üéØYour Mission\n",
    "\n",
    "You‚Äôve been hired by a marketing analytics team for a retail company. They are planning a new customer loyalty program and need clean, analysis-ready data to identify purchase patterns and segment customers effectively.\n",
    "\n",
    "You are given two datasets\n",
    "- customer_retail.csv\n",
    "- transactions_retail.csv\n",
    "\n",
    "Your first task is to prepare both datasets for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc2e01",
   "metadata": {
    "id": "c2fc2e01"
   },
   "source": [
    "By working through this problem, you will apply the following learning objectives from Lessons 3A - 3E:\n",
    "- Learn how to handle missing data effectively.\n",
    "- Perform data manipulation and transformation using Python.\n",
    "- Join datasets for a comprehensive analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b60d81a",
   "metadata": {
    "id": "8b60d81a"
   },
   "source": [
    "## üí≠Apply #AlgoThinking\n",
    "\n",
    "Before diving into the data, let's start by listing down what are the steps you will be taking to handle the this task. (Include atleast 5 steps)"
   ]
  },
  {
   "cell_type": "code",
   "id": "5972e486",
   "metadata": {
    "id": "5972e486",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:47.060623Z",
     "start_time": "2025-09-20T01:34:47.056254Z"
    }
   },
   "source": [
    "# load and inspect the data\n",
    "# handle missing data\n",
    "# clean and transform data\n",
    "# join dataset\n",
    "# last check and save clean data"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "8d654a89",
   "metadata": {
    "id": "8d654a89"
   },
   "source": [
    "## üí°Some helpful tips while you code:\n",
    "\n",
    "You can use the cheatsheet to support you in the process of coding. You may also consider answering the following specific questions that will help you in the process:\n",
    "\n",
    "**1. Handling Missing Data:**\n",
    "- What columns contain missing values?\n",
    "- Should we remove rows with missing values or fill them with a specific value?\n",
    "- What imputation method (mean, median, mode) should we use?\n",
    "\n",
    "**2. Data Manipulation & Transformation:**\n",
    "- Are there any incorrect data types that need conversion?\n",
    "- Do we need to create new columns for better analysis (e.g., total spend per customer)?\n",
    "- How can we standardize categorical data for consistency?\n",
    "\n",
    "**3. Data Joining:**\n",
    "- What is the common key to merge the datasets?\n",
    "- What type of join (inner, left, right, full) should be used?\n",
    "- How do we ensure there are no duplicate records post-merge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af57f543",
   "metadata": {
    "id": "af57f543"
   },
   "source": [
    "## üõ†Ô∏èYour Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J6A4hOCYbtn-",
   "metadata": {
    "id": "J6A4hOCYbtn-"
   },
   "source": [
    "Import the relevant Python libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d57fa2c2",
   "metadata": {
    "id": "d57fa2c2",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:47.155700Z",
     "start_time": "2025-09-20T01:34:47.085060Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from odbc import dataError\n",
    "from tomlkit import value"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "To3SlU1TiAWw",
   "metadata": {
    "id": "To3SlU1TiAWw"
   },
   "source": [
    "Load both datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "45cccb58",
   "metadata": {
    "id": "45cccb58",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:47.263111Z",
     "start_time": "2025-09-20T01:34:47.173356Z"
    }
   },
   "source": [
    "customer = pd.read_csv(\"customer_retail.csv\")\n",
    "customer.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id  age  gender  income   education region loyalty_status  \\\n",
       "0   1   27    Male   40682    Bachelor   East           Gold   \n",
       "1   2   29    Male   15317     Masters   West        Regular   \n",
       "2   3   37    Male   38849    Bachelor   West         Silver   \n",
       "3   4   30    Male   11568  HighSchool  South        Regular   \n",
       "4   5   31  Female   46952     College  North        Regular   \n",
       "\n",
       "  purchase_frequency  purchase_amount product_category  promotion_usage  \\\n",
       "0           frequent            18249            Books                0   \n",
       "1               rare             4557         Clothing                1   \n",
       "2               rare            11822         Clothing                0   \n",
       "3           frequent             4098             Food                0   \n",
       "4         occasional            19685         Clothing                1   \n",
       "\n",
       "   satisfaction_score  \n",
       "0                 6.0  \n",
       "1                 6.0  \n",
       "2                 6.0  \n",
       "3                 7.0  \n",
       "4                 5.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>education</th>\n",
       "      <th>region</th>\n",
       "      <th>loyalty_status</th>\n",
       "      <th>purchase_frequency</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>product_category</th>\n",
       "      <th>promotion_usage</th>\n",
       "      <th>satisfaction_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>Male</td>\n",
       "      <td>40682</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>East</td>\n",
       "      <td>Gold</td>\n",
       "      <td>frequent</td>\n",
       "      <td>18249</td>\n",
       "      <td>Books</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>15317</td>\n",
       "      <td>Masters</td>\n",
       "      <td>West</td>\n",
       "      <td>Regular</td>\n",
       "      <td>rare</td>\n",
       "      <td>4557</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>Male</td>\n",
       "      <td>38849</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>West</td>\n",
       "      <td>Silver</td>\n",
       "      <td>rare</td>\n",
       "      <td>11822</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>Male</td>\n",
       "      <td>11568</td>\n",
       "      <td>HighSchool</td>\n",
       "      <td>South</td>\n",
       "      <td>Regular</td>\n",
       "      <td>frequent</td>\n",
       "      <td>4098</td>\n",
       "      <td>Food</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>Female</td>\n",
       "      <td>46952</td>\n",
       "      <td>College</td>\n",
       "      <td>North</td>\n",
       "      <td>Regular</td>\n",
       "      <td>occasional</td>\n",
       "      <td>19685</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "03522a1e-4972-46f4-a568-fa90c1d339c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:47.371332Z",
     "start_time": "2025-09-20T01:34:47.312270Z"
    }
   },
   "source": [
    "transactions = pd.read_csv(\"transactions_retail.csv\")\n",
    "transactions.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    id  transaction_id transaction_date payment_method  total_spent  \\\n",
       "0  247               1        01/1/2023         PayPal        608.0   \n",
       "1  727               2        02/1/2023           Cash        193.0   \n",
       "2  614               3        03/1/2023         PayPal        815.0   \n",
       "3   56               4        04/1/2023         PayPal        885.0   \n",
       "4  268               5        05/1/2023         PayPal         92.0   \n",
       "\n",
       "   discount_applied  items_purchased return_status  \n",
       "0             180.0                1     No Return  \n",
       "1             112.0                1      Returned  \n",
       "2              56.0                6      Returned  \n",
       "3              86.0                3     No Return  \n",
       "4             101.0                9     No Return  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>payment_method</th>\n",
       "      <th>total_spent</th>\n",
       "      <th>discount_applied</th>\n",
       "      <th>items_purchased</th>\n",
       "      <th>return_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>247</td>\n",
       "      <td>1</td>\n",
       "      <td>01/1/2023</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>608.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1</td>\n",
       "      <td>No Return</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>727</td>\n",
       "      <td>2</td>\n",
       "      <td>02/1/2023</td>\n",
       "      <td>Cash</td>\n",
       "      <td>193.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Returned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>614</td>\n",
       "      <td>3</td>\n",
       "      <td>03/1/2023</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>815.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Returned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>04/1/2023</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>885.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>3</td>\n",
       "      <td>No Return</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>268</td>\n",
       "      <td>5</td>\n",
       "      <td>05/1/2023</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>92.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>9</td>\n",
       "      <td>No Return</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:47.462133Z",
     "start_time": "2025-09-20T01:34:47.418765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Customer Retail Tables Info\\n\", customer.info())\n",
    "print(\"\\n\")\n",
    "print(\"Transactions Retail Tables Info\\n\", customer.info())"
   ],
   "id": "1fadf1a7b0f62cc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   id                  1000 non-null   int64  \n",
      " 1   age                 1000 non-null   int64  \n",
      " 2   gender              1000 non-null   object \n",
      " 3   income              1000 non-null   int64  \n",
      " 4   education           997 non-null    object \n",
      " 5   region              1000 non-null   object \n",
      " 6   loyalty_status      1000 non-null   object \n",
      " 7   purchase_frequency  994 non-null    object \n",
      " 8   purchase_amount     1000 non-null   int64  \n",
      " 9   product_category    1000 non-null   object \n",
      " 10  promotion_usage     1000 non-null   int64  \n",
      " 11  satisfaction_score  993 non-null    float64\n",
      "dtypes: float64(1), int64(5), object(6)\n",
      "memory usage: 93.9+ KB\n",
      "Customer Retail Tables Info\n",
      " None\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   id                  1000 non-null   int64  \n",
      " 1   age                 1000 non-null   int64  \n",
      " 2   gender              1000 non-null   object \n",
      " 3   income              1000 non-null   int64  \n",
      " 4   education           997 non-null    object \n",
      " 5   region              1000 non-null   object \n",
      " 6   loyalty_status      1000 non-null   object \n",
      " 7   purchase_frequency  994 non-null    object \n",
      " 8   purchase_amount     1000 non-null   int64  \n",
      " 9   product_category    1000 non-null   object \n",
      " 10  promotion_usage     1000 non-null   int64  \n",
      " 11  satisfaction_score  993 non-null    float64\n",
      "dtypes: float64(1), int64(5), object(6)\n",
      "memory usage: 93.9+ KB\n",
      "Transactions Retail Tables Info\n",
      " None\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "_BtCZ3bu0CEE",
   "metadata": {
    "id": "_BtCZ3bu0CEE"
   },
   "source": [
    "###Part 1: Handling Missing Data\n",
    "\n",
    "1) Handle the missing data for both datasets\n",
    "\n",
    "üí°Tip: Use print( ) to display the missing data for each dataset to identify which columns need cleaning."
   ]
  },
  {
   "cell_type": "code",
   "id": "c1ea7c77",
   "metadata": {
    "id": "c1ea7c77",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:47.507577Z",
     "start_time": "2025-09-20T01:34:47.497440Z"
    }
   },
   "source": [
    "# total missing data in each column\n",
    "print(\"Missing data in customer_retail table\\n\", customer.isnull().sum())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data in customer_retail table\n",
      " id                    0\n",
      "age                   0\n",
      "gender                0\n",
      "income                0\n",
      "education             3\n",
      "region                0\n",
      "loyalty_status        0\n",
      "purchase_frequency    6\n",
      "purchase_amount       0\n",
      "product_category      0\n",
      "promotion_usage       0\n",
      "satisfaction_score    7\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "a9159685-02f5-455c-88f6-0253a58fc920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:47.581982Z",
     "start_time": "2025-09-20T01:34:47.572009Z"
    }
   },
   "source": [
    "# total missing data in each column\n",
    "print(\"Missing data in transaction_retail table\\n\", transactions.isnull().sum())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data in transaction_retail table\n",
      " id                   0\n",
      "transaction_id       0\n",
      "transaction_date     0\n",
      "payment_method       0\n",
      "total_spent          3\n",
      "discount_applied    11\n",
      "items_purchased      0\n",
      "return_status        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:47.675403Z",
     "start_time": "2025-09-20T01:34:47.640160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print all missing data for each row\n",
    "missing_edu_value_after = customer.groupby('id')[['age', 'education']].first().reset_index()\n",
    "\n",
    "# Filter for the IDs you want\n",
    "missing_edu_value_after = missing_edu_value_after[missing_edu_value_after['id'].isin([18, 19, 20])]\n",
    "\n",
    "print(missing_edu_value_after)\n",
    "# customer = missing_edu_value_after"
   ],
   "id": "e567033da7454ee5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id  age education\n",
      "17  18   21   College\n",
      "18  19   33      None\n",
      "19  20   28      None\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:47.748890Z",
     "start_time": "2025-09-20T01:34:47.734921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print all missing data for each row\n",
    "# display(transactions[transactions.isnull().any(axis=1)])\n",
    "customer.isnull().sum()"
   ],
   "id": "c2289b8c66008cba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    0\n",
       "age                   0\n",
       "gender                0\n",
       "income                0\n",
       "education             3\n",
       "region                0\n",
       "loyalty_status        0\n",
       "purchase_frequency    6\n",
       "purchase_amount       0\n",
       "product_category      0\n",
       "promotion_usage       0\n",
       "satisfaction_score    7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "FcgCOu8NRDop",
   "metadata": {
    "id": "FcgCOu8NRDop"
   },
   "source": [
    "**Missing Values are present in:**\n",
    "\n",
    "1. education (categorical variable)\n",
    "2. purchase frequency (categorical variable)\n",
    "3. satisfaction score (numerical variable)\n",
    "4. total_spent (numerical variable)\n",
    "5. discount_applied (numerical variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3D8A_aRT-5",
   "metadata": {
    "id": "fa3D8A_aRT-5"
   },
   "source": [
    "2) For categorical variables, we can generally remove missing values if the **missingness is completely random and not associated with any other variable.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "KGsfrsDKRre6",
   "metadata": {
    "id": "KGsfrsDKRre6",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:47.836028Z",
     "start_time": "2025-09-20T01:34:47.807712Z"
    }
   },
   "source": [
    "# we will focus on missing data only, if we drop certain does it will effect other data\n",
    "# untuk purchase_frequency xbole nk create sendiri dari mana2 column, so buang macam biasea\n",
    "\n",
    "customer = customer.dropna(subset=['purchase_frequency'])\n",
    "customer.isnull().sum()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    0\n",
       "age                   0\n",
       "gender                0\n",
       "income                0\n",
       "education             3\n",
       "region                0\n",
       "loyalty_status        0\n",
       "purchase_frequency    0\n",
       "purchase_amount       0\n",
       "product_category      0\n",
       "promotion_usage       0\n",
       "satisfaction_score    7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "mD4PBxPLR5OC",
   "metadata": {
    "id": "mD4PBxPLR5OC"
   },
   "source": [
    "Now, no missing value in the **purchase_frequency** column!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3DSwPLVBWhYx",
   "metadata": {
    "id": "3DSwPLVBWhYx"
   },
   "source": [
    "3) If a categorical variable is associated with another variable, we can perform conditional imputation using defined rules or value ranges.\n",
    "\n",
    "For example, we have missing values in the \"education\" column:\n",
    "\n",
    "1. Check the unique categories in the education column.\n",
    "2. Create conditions to correlate with the \"age\" column.\n",
    "3. Perform imputation"
   ]
  },
  {
   "cell_type": "code",
   "id": "zhMD88ZbW-_V",
   "metadata": {
    "id": "zhMD88ZbW-_V",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:47.876946Z",
     "start_time": "2025-09-20T01:34:47.866560Z"
    }
   },
   "source": [
    "# untuk education since secara average populasi manusia mengamnbil certain education level boleh dianggarkan so kita bole tukar based on kategori umur for each education level\n",
    "# what to do:\n",
    "# 1. list all education level\n",
    "# 2. tengok which column yg missing\n",
    "# 3. buat conditions\n",
    "# 4. imputekan\n",
    "\n",
    "# step 1\n",
    "customer['education'].unique()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bachelor', 'Masters', 'HighSchool', 'College', nan], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "R5tfmVHCXr5t",
   "metadata": {
    "id": "R5tfmVHCXr5t",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:47.939889Z",
     "start_time": "2025-09-20T01:34:47.927216Z"
    }
   },
   "source": [
    "# step 2: check missing values\n",
    "raw_education = customer[customer['education'].isnull()]\n",
    "print(raw_education[['age','education']])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age education\n",
      "18   33       NaN\n",
      "19   28       NaN\n",
      "20   25       NaN\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:48.033441Z",
     "start_time": "2025-09-20T01:34:47.979166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# step 3\n",
    "# customer['educational_original'] = customer['education']\n",
    "\n",
    "# anggarkan missing education based on educationlevel\n",
    "def infer_education(age):\n",
    "    if pd.isnull(age):\n",
    "        return np.nan\n",
    "    elif age < 22:\n",
    "        return 'College'\n",
    "    elif 22 <= age < 30:\n",
    "        return 'Bachelor'\n",
    "    elif 30 <= age <= 40:\n",
    "        return 'Masters'\n",
    "    else:\n",
    "        return 'Bachelor'\n",
    "\n",
    "# Fill missing values\n",
    "customer['education'] = customer['education'].fillna(customer['age'].apply(infer_education))\n",
    "\n",
    "# imputed_rows = customer[customer['educational_original'].isnull()]\n",
    "# print(imputed_rows[['age', 'education']])\n",
    "\n",
    "# step 4\n",
    "display(customer)\n",
    "missing_value = customer['education'].isnull().sum()\n",
    "print(\"\\nNo missing value in education column\" if missing_value == 0 else \"There is \"+str(missing_value)+\" missing value in education column\")"
   ],
   "id": "c4694055d4ae2457",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       id  age  gender  income   education region loyalty_status  \\\n",
       "0       1   27    Male   40682    Bachelor   East           Gold   \n",
       "1       2   29    Male   15317     Masters   West        Regular   \n",
       "2       3   37    Male   38849    Bachelor   West         Silver   \n",
       "3       4   30    Male   11568  HighSchool  South        Regular   \n",
       "4       5   31  Female   46952     College  North        Regular   \n",
       "..    ...  ...     ...     ...         ...    ...            ...   \n",
       "995   996   30  Female   18444     College   West        Regular   \n",
       "996   997   35    Male   23348    Bachelor  South        Regular   \n",
       "997   998   24    Male    7627  HighSchool   West        Regular   \n",
       "998   999   28    Male   47063    Bachelor   West         Silver   \n",
       "999  1000   29  Female    5688    Bachelor   East        Regular   \n",
       "\n",
       "    purchase_frequency  purchase_amount product_category  promotion_usage  \\\n",
       "0             frequent            18249            Books                0   \n",
       "1                 rare             4557         Clothing                1   \n",
       "2                 rare            11822         Clothing                0   \n",
       "3             frequent             4098             Food                0   \n",
       "4           occasional            19685         Clothing                1   \n",
       "..                 ...              ...              ...              ...   \n",
       "995           frequent             8090      Electronics                0   \n",
       "996           frequent             7912            Books                1   \n",
       "997         occasional             2561      Electronics                0   \n",
       "998         occasional            19203            Books                0   \n",
       "999         occasional             1854      Electronics                1   \n",
       "\n",
       "     satisfaction_score  \n",
       "0                   6.0  \n",
       "1                   6.0  \n",
       "2                   6.0  \n",
       "3                   7.0  \n",
       "4                   5.0  \n",
       "..                  ...  \n",
       "995                 4.0  \n",
       "996                 6.0  \n",
       "997                 5.0  \n",
       "998                 6.0  \n",
       "999                 5.0  \n",
       "\n",
       "[994 rows x 12 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>education</th>\n",
       "      <th>region</th>\n",
       "      <th>loyalty_status</th>\n",
       "      <th>purchase_frequency</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>product_category</th>\n",
       "      <th>promotion_usage</th>\n",
       "      <th>satisfaction_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>Male</td>\n",
       "      <td>40682</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>East</td>\n",
       "      <td>Gold</td>\n",
       "      <td>frequent</td>\n",
       "      <td>18249</td>\n",
       "      <td>Books</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>15317</td>\n",
       "      <td>Masters</td>\n",
       "      <td>West</td>\n",
       "      <td>Regular</td>\n",
       "      <td>rare</td>\n",
       "      <td>4557</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>Male</td>\n",
       "      <td>38849</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>West</td>\n",
       "      <td>Silver</td>\n",
       "      <td>rare</td>\n",
       "      <td>11822</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>Male</td>\n",
       "      <td>11568</td>\n",
       "      <td>HighSchool</td>\n",
       "      <td>South</td>\n",
       "      <td>Regular</td>\n",
       "      <td>frequent</td>\n",
       "      <td>4098</td>\n",
       "      <td>Food</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>Female</td>\n",
       "      <td>46952</td>\n",
       "      <td>College</td>\n",
       "      <td>North</td>\n",
       "      <td>Regular</td>\n",
       "      <td>occasional</td>\n",
       "      <td>19685</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>996</td>\n",
       "      <td>30</td>\n",
       "      <td>Female</td>\n",
       "      <td>18444</td>\n",
       "      <td>College</td>\n",
       "      <td>West</td>\n",
       "      <td>Regular</td>\n",
       "      <td>frequent</td>\n",
       "      <td>8090</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>997</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>23348</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>South</td>\n",
       "      <td>Regular</td>\n",
       "      <td>frequent</td>\n",
       "      <td>7912</td>\n",
       "      <td>Books</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>998</td>\n",
       "      <td>24</td>\n",
       "      <td>Male</td>\n",
       "      <td>7627</td>\n",
       "      <td>HighSchool</td>\n",
       "      <td>West</td>\n",
       "      <td>Regular</td>\n",
       "      <td>occasional</td>\n",
       "      <td>2561</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>999</td>\n",
       "      <td>28</td>\n",
       "      <td>Male</td>\n",
       "      <td>47063</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>West</td>\n",
       "      <td>Silver</td>\n",
       "      <td>occasional</td>\n",
       "      <td>19203</td>\n",
       "      <td>Books</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1000</td>\n",
       "      <td>29</td>\n",
       "      <td>Female</td>\n",
       "      <td>5688</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>East</td>\n",
       "      <td>Regular</td>\n",
       "      <td>occasional</td>\n",
       "      <td>1854</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>994 rows √ó 12 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No missing value in education column\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5b0c9fd4f8002c30"
  },
  {
   "cell_type": "markdown",
   "id": "BFAX9rhUl7yA",
   "metadata": {
    "id": "BFAX9rhUl7yA"
   },
   "source": [
    "Note: We will apply some of what we learned from **Lesson 2B: Intro to Conditionals** and **Lesson 2F: Functions in Python** in the following codes."
   ]
  },
  {
   "cell_type": "code",
   "id": "vjyGu3DHXEC1",
   "metadata": {
    "id": "vjyGu3DHXEC1",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:48.082117Z",
     "start_time": "2025-09-20T01:34:48.073692Z"
    }
   },
   "source": "# okay?",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "0V1E3Mp9lV-P",
   "metadata": {
    "id": "0V1E3Mp9lV-P"
   },
   "source": [
    "In applied data science, imputations are normally guided by established best-practice rules. However, for this exercise we will proceed with a few simplifying assumptions where we correlate age with education level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VFhVB8x4SBpx",
   "metadata": {
    "id": "VFhVB8x4SBpx"
   },
   "source": [
    "4) When imputing for numerical columns, it is best practice to check for outliers.\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Usually for numerical columns, we want to impute with either the mean or median value. Outliers can distort the *mean*, making it a poor choice for imputation. The median is more robust to outliers and gives a better central value when they are present.\n",
    "\n",
    "**What to do depending on outliers:**\n",
    "\n",
    "A) Impute with **Mean** ‚Äì if no outliers\n",
    "- If a numerical column does not have outliers, it's safe to replace missing values with the mean (average) of the column.\n",
    "\n",
    "B) Impute with **Median** ‚Äì if outliers are present\n",
    "If a numerical column has outliers, use the median instead of the mean.\n",
    "\n",
    "- The median is the middle value and is not affected by extreme values, so it gives a more reliable imputation.\n",
    "\n",
    "\n",
    "**For more information about outlier detection:** https://blog.alliedoffsets.com/beyond-the-norm-how-outlier-detection-transforms-data-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yrGB5kV9TWvT",
   "metadata": {
    "id": "yrGB5kV9TWvT"
   },
   "source": [
    "Let's check if the column with missing value (satisfaction_score) in the customers data contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "id": "rSPk6xc2S25K",
   "metadata": {
    "id": "rSPk6xc2S25K",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:48.138034Z",
     "start_time": "2025-09-20T01:34:48.110876Z"
    }
   },
   "source": [
    "def check_outlier(col):\n",
    "    Q1 = col.quantile(0.25)\n",
    "    Q3 = col.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # this will be determine outlier\n",
    "    # define lower and upper bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = []\n",
    "    for value in col:\n",
    "        if value < lower_bound or value > upper_bound:\n",
    "            outliers.append(value)\n",
    "\n",
    "    return len(outliers), outliers\n",
    "\n",
    "print(customer['satisfaction_score'].describe())\n",
    "print(\"Number of outlier for statisfaction_score:\", check_outlier(customer['satisfaction_score']))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    987.000000\n",
      "mean       4.954407\n",
      "std        1.030443\n",
      "min        2.000000\n",
      "25%        4.000000\n",
      "50%        5.000000\n",
      "75%        6.000000\n",
      "max        8.000000\n",
      "Name: satisfaction_score, dtype: float64\n",
      "Number of outlier for statisfaction_score: (0, [])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:48.184305Z",
     "start_time": "2025-09-20T01:34:48.162309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# different approach\n",
    "import numpy as np #to determine outliers for numerical data\n",
    "\n",
    "# Select numerical columns with missing values\n",
    "numerical_cols_with_na = customer.select_dtypes(include=np.number).columns[\n",
    "    customer.select_dtypes(include=np.number).isnull().any()\n",
    "]\n",
    "\n",
    "# Dictionary to store results\n",
    "outlier_flags = {}\n",
    "\n",
    "# Check for outliers using IQR method\n",
    "for col in numerical_cols_with_na:\n",
    "    Q1 = customer[col].quantile(0.25)\n",
    "    Q3 = customer[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Check if any values are outliers (excluding NaNs)\n",
    "    outliers_exist = ((customer[col] < lower_bound) | (customer[col] > upper_bound)).any()\n",
    "\n",
    "    # Store result\n",
    "    outlier_flags[col] = outliers_exist\n",
    "\n",
    "# Show True/False for each column\n",
    "print(outlier_flags)"
   ],
   "id": "99e8f4e027ffee78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'satisfaction_score': np.False_}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "SECSr-2FTe5O",
   "metadata": {
    "id": "SECSr-2FTe5O"
   },
   "source": [
    "Check if the columns with missing value (total_spent and discount_applied) in the transactions data contain outliers."
   ]
  },
  {
   "cell_type": "code",
   "id": "24kVe5MMTKHG",
   "metadata": {
    "id": "24kVe5MMTKHG",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:48.282635Z",
     "start_time": "2025-09-20T01:34:48.247779Z"
    }
   },
   "source": [
    "\n",
    "print(\"Number of outlier for total_spent:\", check_outlier(transactions['total_spent']))\n",
    "\n",
    "print(\"Number of outlier for discount_applied:\", check_outlier(transactions['discount_applied']))\n",
    "transactions['discount_applied'].describe()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outlier for total_spent: (0, [])\n",
      "Number of outlier for discount_applied: (3, [1850.0, 1340.0, 1333.0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     739.000000\n",
       "mean      106.288227\n",
       "std       107.823074\n",
       "min         0.000000\n",
       "25%        50.000000\n",
       "50%        99.000000\n",
       "75%       153.000000\n",
       "max      1850.000000\n",
       "Name: discount_applied, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "koengi310F-o",
   "metadata": {
    "id": "koengi310F-o"
   },
   "source": [
    "Now there are no outliers in satisfaction_score and total_spent, we can impute with \"mean\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "aed78962",
   "metadata": {
    "id": "aed78962",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:48.462723Z",
     "start_time": "2025-09-20T01:34:48.440395Z"
    }
   },
   "source": [
    "def impute_data(col,method='mean'):\n",
    "    if method == 'mean':\n",
    "        return col.fillna(col.mean())\n",
    "    elif method == 'median':\n",
    "        return col.fillna(col.median())\n",
    "\n",
    "customer['satisfaction_score'] = impute_data(customer['satisfaction_score'], method='mean')\n",
    "transactions['total_spent'] = impute_data(transactions['total_spent'], method='mean')\n",
    "\n",
    "print(customer[['satisfaction_score']].isnull().sum())\n",
    "transactions[['total_spent']].isnull().sum()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "satisfaction_score    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "total_spent    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "az1pnrA0VKs5",
   "metadata": {
    "id": "az1pnrA0VKs5"
   },
   "source": [
    "Now, **no missing values** in satisfaction_score and total_spent columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3NjZ5hdhVUgJ",
   "metadata": {
    "id": "3NjZ5hdhVUgJ"
   },
   "source": [
    "There are outliers in the discount_applied column, so we can impute with \"median\""
   ]
  },
  {
   "cell_type": "code",
   "id": "eUKFGGtuVeL_",
   "metadata": {
    "id": "eUKFGGtuVeL_",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:48.535523Z",
     "start_time": "2025-09-20T01:34:48.521451Z"
    }
   },
   "source": [
    "transactions['discount_applied'] = impute_data(transactions['discount_applied'], method='median')\n",
    "transactions[['discount_applied']].isnull().sum()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discount_applied    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "DhRHdt4M0h3k",
   "metadata": {
    "id": "DhRHdt4M0h3k"
   },
   "source": [
    "###Part 2: Data Manipulation and Transformation\n",
    "\n",
    "1) Now that our data have been cleaned, we can safely join the tables. Merging the datasets first, then applying further transformations, will let us review every column and make informed decisions about which custom columns to create."
   ]
  },
  {
   "cell_type": "code",
   "id": "b7130709",
   "metadata": {
    "id": "b7130709",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:48.605123Z",
     "start_time": "2025-09-20T01:34:48.589141Z"
    }
   },
   "source": "customer.isnull().sum()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    0\n",
       "age                   0\n",
       "gender                0\n",
       "income                0\n",
       "education             0\n",
       "region                0\n",
       "loyalty_status        0\n",
       "purchase_frequency    0\n",
       "purchase_amount       0\n",
       "product_category      0\n",
       "promotion_usage       0\n",
       "satisfaction_score    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:48.676645Z",
     "start_time": "2025-09-20T01:34:48.662597Z"
    }
   },
   "cell_type": "code",
   "source": "transactions.isnull().sum()",
   "id": "48b5d35d6fd27c36",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                  0\n",
       "transaction_id      0\n",
       "transaction_date    0\n",
       "payment_method      0\n",
       "total_spent         0\n",
       "discount_applied    0\n",
       "items_purchased     0\n",
       "return_status       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:48.784673Z",
     "start_time": "2025-09-20T01:34:48.751244Z"
    }
   },
   "cell_type": "code",
   "source": "merged_df = transactions.merge(customer, on='id', how='inner')",
   "id": "160c9662b9a07392",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:48.851135Z",
     "start_time": "2025-09-20T01:34:48.813650Z"
    }
   },
   "cell_type": "code",
   "source": "display(merged_df[merged_df.isnull().any(axis=1)])",
   "id": "18245c0ab5a53d3e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, transaction_id, transaction_date, payment_method, total_spent, discount_applied, items_purchased, return_status, age, gender, income, education, region, loyalty_status, purchase_frequency, purchase_amount, product_category, promotion_usage, satisfaction_score]\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>payment_method</th>\n",
       "      <th>total_spent</th>\n",
       "      <th>discount_applied</th>\n",
       "      <th>items_purchased</th>\n",
       "      <th>return_status</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>education</th>\n",
       "      <th>region</th>\n",
       "      <th>loyalty_status</th>\n",
       "      <th>purchase_frequency</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>product_category</th>\n",
       "      <th>promotion_usage</th>\n",
       "      <th>satisfaction_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "DQmVrz5J0nMl",
   "metadata": {
    "id": "DQmVrz5J0nMl"
   },
   "source": [
    "2) Now that the data has been merged, we will perform some data manipulation to create custom columns that can be useful for Exploratory Data Analysis.\n",
    "\n",
    "In this case, we will:\n",
    "\n",
    "a. Create a custom column called 'score_rank' on the 'score' column\n",
    "\n",
    "b. Apply the function to create the new column\n",
    "\n",
    "c. Display the updated DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "id": "bdaa4f27",
   "metadata": {
    "id": "bdaa4f27",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:48.920925Z",
     "start_time": "2025-09-20T01:34:48.898007Z"
    }
   },
   "source": [
    "# b\n",
    "def rank_score(score):\n",
    "    if pd.isnull(score):\n",
    "        return 'Missing'\n",
    "    elif score < 4:\n",
    "        return 'Low'\n",
    "    elif 4 <= score < 7:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "# a and b\n",
    "merged_df['score_rank'] = merged_df['satisfaction_score'].apply(rank_score)\n",
    "\n",
    "# c\n",
    "print(merged_df[['satisfaction_score', 'score_rank']].head())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   satisfaction_score score_rank\n",
      "0                 6.0    Neutral\n",
      "1                 5.0    Neutral\n",
      "2                 4.0    Neutral\n",
      "3                 7.0       High\n",
      "4                 4.0    Neutral\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "7n0W3Qt2oJ8k",
   "metadata": {
    "id": "7n0W3Qt2oJ8k"
   },
   "source": [
    "3) **üë®‚Äçüë©‚Äçüëß‚ÄçüëßGroup Work:**\n",
    "\n",
    "What other custom columns can you create that could be useful for Exploratory Data Analysis?\n",
    "\n",
    "Discuss in groups and come up with some ways to do so. Share your approach and code with the rest of the class."
   ]
  },
  {
   "cell_type": "code",
   "id": "cWDXYHZ9oa42",
   "metadata": {
    "id": "cWDXYHZ9oa42",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:49.026748Z",
     "start_time": "2025-09-20T01:34:48.993827Z"
    }
   },
   "source": [
    "# spending behaviour - Not all customers spend the same bcs some are ‚Äúbig spenders,‚Äù others are ‚Äúlow spenders.‚Äù\n",
    "merged_df['spending_category'] = pd.cut(\n",
    "    merged_df['total_spent'],\n",
    "    bins=[0, 50, 200, float('inf')],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "merged_df['spending_category'].head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      High\n",
       "1    Medium\n",
       "2      High\n",
       "3      High\n",
       "4    Medium\n",
       "Name: spending_category, dtype: category\n",
       "Categories (3, object): ['Low' < 'Medium' < 'High']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:49.119021Z",
     "start_time": "2025-09-20T01:34:49.086565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Discount Usage - Discounts influence customer decisions. We want to know who buys only with discounts and who pays full price.\n",
    "merged_df['used_discount'] = merged_df['discount_applied'].apply(lambda x: 'Yes' if x > 0 else 'No')\n",
    "merged_df['discount_ratio'] = merged_df['discount_applied'] / (merged_df['total_spent'] + 1e-5)\n",
    "\n",
    "merged_df[['used_discount','discount_ratio']].head()"
   ],
   "id": "5e79bc79e06f5941",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  used_discount  discount_ratio\n",
       "0           Yes        0.296053\n",
       "1           Yes        0.580311\n",
       "2           Yes        0.068712\n",
       "3           Yes        0.097175\n",
       "4           Yes        1.097826"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>used_discount</th>\n",
       "      <th>discount_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>0.296053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>0.580311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>0.068712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>0.097175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1.097826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:49.367229Z",
     "start_time": "2025-09-20T01:34:49.340188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Customer Demographics - Age influences education, spending power, and product preferences.\n",
    "merged_df['age_group'] = pd.cut(\n",
    "    merged_df['age'],\n",
    "    bins=[0, 18, 30, 50, 100],\n",
    "    labels=['Teen', 'Young Adult', 'Adult', 'Senior']\n",
    ")\n",
    "merged_df['age_group'].head()"
   ],
   "id": "ff1b64e6a795fd8c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Young Adult\n",
       "1          Adult\n",
       "2    Young Adult\n",
       "3          Adult\n",
       "4    Young Adult\n",
       "Name: age_group, dtype: category\n",
       "Categories (4, object): ['Teen' < 'Young Adult' < 'Adult' < 'Senior']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:49.444122Z",
     "start_time": "2025-09-20T01:34:49.425077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Satisfaction Insights - Raw satisfaction scores (1‚Äì5) are harder to interpret. Categorizing makes analysis clearer.\n",
    "merged_df['satisfaction_flag'] = merged_df['satisfaction_score'].apply(\n",
    "    lambda x: 'High' if x >= 4 else ('Low' if x <= 2 else 'Medium')\n",
    ")\n",
    "merged_df['satisfaction_flag'].head()"
   ],
   "id": "a95baa3a13caebd8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    High\n",
       "1    High\n",
       "2    High\n",
       "3    High\n",
       "4    High\n",
       "Name: satisfaction_flag, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:49.537849Z",
     "start_time": "2025-09-20T01:34:49.510102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Engagement Features - Some customers shop once, others shop many times. Frequency matters as much as spending.\n",
    "customer_summary = merged_df.groupby('id').agg({\n",
    "    'total_spent': 'sum',\n",
    "    'discount_applied': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "}).rename(columns={'transaction_id': 'num_transactions'})\n",
    "print(customer_summary)\n",
    "\n"
   ],
   "id": "2ace26c15588b7f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      total_spent  discount_applied  num_transactions\n",
      "id                                                   \n",
      "1           404.0              26.0                 1\n",
      "2           490.0              54.0                 1\n",
      "3           348.0               9.0                 1\n",
      "5           272.0             130.0                 1\n",
      "7           823.0              97.0                 1\n",
      "...           ...               ...               ...\n",
      "995         924.0              85.0                 1\n",
      "996         582.0             149.0                 1\n",
      "997         936.0              37.0                 1\n",
      "998         442.0             115.0                 1\n",
      "1000        601.0              25.0                 1\n",
      "\n",
      "[746 rows x 3 columns]\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "8rfmXgcoodK8",
   "metadata": {
    "id": "8rfmXgcoodK8"
   },
   "source": [
    "4) Let's export our merged data as merged_data_retail.csv. Please save this file - we will return to it during Lesson 4S: Sync Session to perform further Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "code",
   "id": "Fd7hWB-SaWjg",
   "metadata": {
    "id": "Fd7hWB-SaWjg",
    "ExecuteTime": {
     "end_time": "2025-09-20T01:34:49.686857Z",
     "start_time": "2025-09-20T01:34:49.607767Z"
    }
   },
   "source": [
    "merged_df.to_csv(\"retail.csv\", index=False)\n",
    "print(f\"Cleaned dataset has been successfully saved \")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset has been successfully saved \n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T01:35:34.613608Z",
     "start_time": "2025-09-20T01:34:49.797374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "df=sns.load_dataset(\"iris\")\n",
    "df.head()"
   ],
   "id": "37fc8919b8d82d07",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTimeoutError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\Lib\\socket.py:849\u001B[0m, in \u001B[0;36mcreate_connection\u001B[1;34m(address, timeout, source_address, all_errors)\u001B[0m\n\u001B[0;32m    848\u001B[0m     sock\u001B[38;5;241m.\u001B[39mbind(source_address)\n\u001B[1;32m--> 849\u001B[0m sock\u001B[38;5;241m.\u001B[39mconnect(sa)\n\u001B[0;32m    850\u001B[0m \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n",
      "\u001B[1;31mTimeoutError\u001B[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mseaborn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msns\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m df\u001B[38;5;241m=\u001B[39msns\u001B[38;5;241m.\u001B[39mload_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miris\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m df\u001B[38;5;241m.\u001B[39mhead()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\seaborn\\utils.py:572\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(name, cache, data_home, **kws)\u001B[0m\n\u001B[0;32m    570\u001B[0m cache_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(get_data_home(data_home), os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(url))\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(cache_path):\n\u001B[1;32m--> 572\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m get_dataset_names():\n\u001B[0;32m    573\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not one of the example datasets.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    574\u001B[0m     urlretrieve(url, cache_path)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\seaborn\\utils.py:499\u001B[0m, in \u001B[0;36mget_dataset_names\u001B[1;34m()\u001B[0m\n\u001B[0;32m    493\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_dataset_names\u001B[39m():\n\u001B[0;32m    494\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Report available example datasets, useful for reporting issues.\u001B[39;00m\n\u001B[0;32m    495\u001B[0m \n\u001B[0;32m    496\u001B[0m \u001B[38;5;124;03m    Requires an internet connection.\u001B[39;00m\n\u001B[0;32m    497\u001B[0m \n\u001B[0;32m    498\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 499\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m urlopen(DATASET_NAMES_URL) \u001B[38;5;28;01mas\u001B[39;00m resp:\n\u001B[0;32m    500\u001B[0m         txt \u001B[38;5;241m=\u001B[39m resp\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m    502\u001B[0m     dataset_names \u001B[38;5;241m=\u001B[39m [name\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m txt\u001B[38;5;241m.\u001B[39mdecode()\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\urllib\\request.py:189\u001B[0m, in \u001B[0;36murlopen\u001B[1;34m(url, data, timeout, context)\u001B[0m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    188\u001B[0m     opener \u001B[38;5;241m=\u001B[39m _opener\n\u001B[1;32m--> 189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m opener\u001B[38;5;241m.\u001B[39mopen(url, data, timeout)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\urllib\\request.py:489\u001B[0m, in \u001B[0;36mOpenerDirector.open\u001B[1;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[0;32m    486\u001B[0m     req \u001B[38;5;241m=\u001B[39m meth(req)\n\u001B[0;32m    488\u001B[0m sys\u001B[38;5;241m.\u001B[39maudit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124murllib.Request\u001B[39m\u001B[38;5;124m'\u001B[39m, req\u001B[38;5;241m.\u001B[39mfull_url, req\u001B[38;5;241m.\u001B[39mdata, req\u001B[38;5;241m.\u001B[39mheaders, req\u001B[38;5;241m.\u001B[39mget_method())\n\u001B[1;32m--> 489\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_open(req, data)\n\u001B[0;32m    491\u001B[0m \u001B[38;5;66;03m# post-process response\u001B[39;00m\n\u001B[0;32m    492\u001B[0m meth_name \u001B[38;5;241m=\u001B[39m protocol\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_response\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\urllib\\request.py:506\u001B[0m, in \u001B[0;36mOpenerDirector._open\u001B[1;34m(self, req, data)\u001B[0m\n\u001B[0;32m    503\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[0;32m    505\u001B[0m protocol \u001B[38;5;241m=\u001B[39m req\u001B[38;5;241m.\u001B[39mtype\n\u001B[1;32m--> 506\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_chain(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_open, protocol, protocol \u001B[38;5;241m+\u001B[39m\n\u001B[0;32m    507\u001B[0m                           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_open\u001B[39m\u001B[38;5;124m'\u001B[39m, req)\n\u001B[0;32m    508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result:\n\u001B[0;32m    509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\urllib\\request.py:466\u001B[0m, in \u001B[0;36mOpenerDirector._call_chain\u001B[1;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[0;32m    464\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers:\n\u001B[0;32m    465\u001B[0m     func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[1;32m--> 466\u001B[0m     result \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m    467\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    468\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1367\u001B[0m, in \u001B[0;36mHTTPSHandler.https_open\u001B[1;34m(self, req)\u001B[0m\n\u001B[0;32m   1366\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mhttps_open\u001B[39m(\u001B[38;5;28mself\u001B[39m, req):\n\u001B[1;32m-> 1367\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_open(http\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mHTTPSConnection, req,\n\u001B[0;32m   1368\u001B[0m                         context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_context)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1319\u001B[0m, in \u001B[0;36mAbstractHTTPHandler.do_open\u001B[1;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[0;32m   1317\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1318\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1319\u001B[0m         h\u001B[38;5;241m.\u001B[39mrequest(req\u001B[38;5;241m.\u001B[39mget_method(), req\u001B[38;5;241m.\u001B[39mselector, req\u001B[38;5;241m.\u001B[39mdata, headers,\n\u001B[0;32m   1320\u001B[0m                   encode_chunked\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39mhas_header(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTransfer-encoding\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m   1321\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n\u001B[0;32m   1322\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m URLError(err)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:1338\u001B[0m, in \u001B[0;36mHTTPConnection.request\u001B[1;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[0;32m   1335\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrequest\u001B[39m(\u001B[38;5;28mself\u001B[39m, method, url, body\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, headers\u001B[38;5;241m=\u001B[39m{}, \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   1336\u001B[0m             encode_chunked\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m   1337\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1338\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:1384\u001B[0m, in \u001B[0;36mHTTPConnection._send_request\u001B[1;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[0;32m   1380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(body, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1381\u001B[0m     \u001B[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001B[39;00m\n\u001B[0;32m   1382\u001B[0m     \u001B[38;5;66;03m# default charset of iso-8859-1.\u001B[39;00m\n\u001B[0;32m   1383\u001B[0m     body \u001B[38;5;241m=\u001B[39m _encode(body, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m-> 1384\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mendheaders(body, encode_chunked\u001B[38;5;241m=\u001B[39mencode_chunked)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:1333\u001B[0m, in \u001B[0;36mHTTPConnection.endheaders\u001B[1;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[0;32m   1331\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1332\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[1;32m-> 1333\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_output(message_body, encode_chunked\u001B[38;5;241m=\u001B[39mencode_chunked)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:1093\u001B[0m, in \u001B[0;36mHTTPConnection._send_output\u001B[1;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[0;32m   1091\u001B[0m msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer)\n\u001B[0;32m   1092\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer[:]\n\u001B[1;32m-> 1093\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(msg)\n\u001B[0;32m   1095\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1096\u001B[0m \n\u001B[0;32m   1097\u001B[0m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n\u001B[0;32m   1098\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(message_body, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m   1099\u001B[0m         \u001B[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m         \u001B[38;5;66;03m# is needed to allow the current position of mmap'ed\u001B[39;00m\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;66;03m# files to be taken into account.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:1037\u001B[0m, in \u001B[0;36mHTTPConnection.send\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m   1035\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1036\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_open:\n\u001B[1;32m-> 1037\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconnect()\n\u001B[0;32m   1038\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1039\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m NotConnected()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:1472\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1469\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1470\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnect to a host on a given (SSL) port.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1472\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mconnect()\n\u001B[0;32m   1474\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tunnel_host:\n\u001B[0;32m   1475\u001B[0m         server_hostname \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tunnel_host\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:1003\u001B[0m, in \u001B[0;36mHTTPConnection.connect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1001\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001B[39;00m\n\u001B[0;32m   1002\u001B[0m sys\u001B[38;5;241m.\u001B[39maudit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp.client.connect\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mport)\n\u001B[1;32m-> 1003\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection(\n\u001B[0;32m   1004\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mport), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_address)\n\u001B[0;32m   1005\u001B[0m \u001B[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001B[39;00m\n\u001B[0;32m   1006\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\socket.py:856\u001B[0m, in \u001B[0;36mcreate_connection\u001B[1;34m(address, timeout, source_address, all_errors)\u001B[0m\n\u001B[0;32m    854\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m error \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    855\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m all_errors:\n\u001B[1;32m--> 856\u001B[0m         exceptions\u001B[38;5;241m.\u001B[39mclear()  \u001B[38;5;66;03m# raise only the last error\u001B[39;00m\n\u001B[0;32m    857\u001B[0m     exceptions\u001B[38;5;241m.\u001B[39mappend(exc)\n\u001B[0;32m    858\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sock \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d638e4e429e7312e"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
